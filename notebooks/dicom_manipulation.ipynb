{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d5a20-757a-401b-b283-fa188c42be61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib widget\n",
    "#%matplotlib ipympl\n",
    "\n",
    "#%reload_ext tensorboard\n",
    "#%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd98c0b-5e5c-4549-98e8-d604b8b111ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle, subprocess\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.ndimage import label as scipy_label\n",
    "import torch\n",
    "import sklearn\n",
    "import csv\n",
    "import gc\n",
    "import pydicom\n",
    "import networkx as nx\n",
    "import copy\n",
    "import scipy\n",
    "#from radiomics import featureextractor\n",
    "#import radiomics\n",
    "\n",
    "import glob\n",
    "from platipy.imaging import ImageVisualiser\n",
    "from platipy.dicom.io.rtstruct_to_nifti import convert_rtstruct, read_dicom_image\n",
    "\n",
    "#%matplotlib notebook\n",
    "#%matplotlib widget\n",
    "#plt.ion()\n",
    "#import initial_ml as iml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e2fc6-966e-4a72-b809-349a7405e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_directory = '../../data/LIDC-IDRI/'\n",
    "out_directory = '../../data/LIDC-IDRI/ct_slices/'\n",
    "nii_directory = '../../data/LIDC-IDRI/Nii'\n",
    "\n",
    "data_path = Path(data_directory)\n",
    "out_path = Path(out_directory)\n",
    "nii_path = Path(nii_directory)\n",
    "embedding_path = data_path.joinpath('embeddings')\n",
    "\n",
    "embedding_path.mkdir(exist_ok=True, parents=True)\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "nii_path.mkdir(exist_ok=True, parents=True)\n",
    "meta_ct_df = pd.read_csv(data_path.joinpath('ct_dicom/metadata.csv'))\n",
    "meta_seg_df = pd.read_csv(data_path.joinpath('seg_dicom/metadata.csv'))\n",
    "\n",
    "meta_ct_df.set_index('Subject ID', inplace=True)\n",
    "meta_seg_df.set_index('Subject ID', inplace=True)\n",
    "\n",
    "ct_slices = pd.read_pickle('./ct_slices_dict.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df27f0-0295-4a78-b39e-2149cdaea2f2",
   "metadata": {},
   "source": [
    "## Slice extraction\n",
    "The following blocks are for collecting the slice information from the SEG files.\n",
    "It goes through each SEG file and gets the Instance UIDs for the marked slices, and then keeps only the unique UIDs (since it is looking through multiple SEGs for the same nodes).\n",
    "Additionally it calculates the center of mass for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d89e662-2d70-479a-9efc-277d85ec3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull uids for segment slices - to be used to pull corresponding slices from the CT images\n",
    "segment_slices = {}\n",
    "segment_com = {}\n",
    "for pat, df_group in tqdm(meta_seg_df.groupby(\"Subject ID\")):\n",
    "    segment_slices[pat] = {}\n",
    "    segment_com[pat] = {}\n",
    "    # Select the structure set with the later date\n",
    "    for idx, seg_row in df_group[df_group.Modality == \"SEG\"].iterrows():\n",
    "        seg_dir = data_path.joinpath('seg_dicom').joinpath(seg_row[\"File Location\"].replace('\\\\','/')) \n",
    "        seg_num = seg_dir.as_posix().split('/')[-1].split('-')[1].split(' ')[-2]\n",
    "        segment_slices[pat][seg_num] = []\n",
    "        segment_com[pat][seg_num] = []\n",
    "    for idx, seg_row in df_group[df_group.Modality == \"SEG\"].iterrows():\n",
    "        seg_dir = data_path.joinpath('seg_dicom').joinpath(seg_row[\"File Location\"].replace('\\\\','/')) \n",
    "\n",
    "        seg_num = seg_dir.as_posix().split('/')[-1].split('-')[1].split(' ')[-2]\n",
    "        seg_file = pydicom.dcmread(seg_dir.joinpath('1-1.dcm'))\n",
    "\n",
    "        segment_com[pat][seg_num].append(np.array(scipy.ndimage.center_of_mass(seg_file.pixel_array)))\n",
    "        #print(segment_com[pat][seg_num])\n",
    "        for uid_source in seg_file.ReferencedSeriesSequence[0].ReferencedInstanceSequence:\n",
    "            segment_slices[pat][seg_num].append(str(uid_source.ReferencedSOPInstanceUID))\n",
    "\n",
    "    for pat in segment_slices.keys():\n",
    "        for seg_num in segment_slices[pat].keys():\n",
    "            segment_slices[pat][seg_num] = np.unique(segment_slices[pat][seg_num])\n",
    "            #print(segment_slices[pat][seg_num])\n",
    "            n_slices = len(segment_slices[pat][seg_num])\n",
    "            com_tmp = segment_com[pat][seg_num]\n",
    "            #print(com_tmp)\n",
    "            new_com = []\n",
    "            for idx, com in enumerate(com_tmp):\n",
    "                #print(com)\n",
    "                if len(com) < 3:\n",
    "                    new_com.append(np.array([np.float64(n_slices/2.), com[0], com[1]]))\n",
    "                else:\n",
    "                    new_com.append(np.array([np.float64(n_slices/2.), com[1], com[2]]))\n",
    "\n",
    "            segment_com[pat][seg_num] = new_com\n",
    "            #segment_com[pat][seg_num] = np.array(segment_com[pat][seg_num]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f409724-2d27-4dfb-98e5-462c25bdbf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrane the COMs so that they are consistent in dimension, and reduce the number of COMs to one COM per node.\n",
    "for pat in segment_com.keys():\n",
    "    print(pat)\n",
    "    for node in segment_com[pat].keys():\n",
    "        print(f'    {node}')\n",
    "        print(segment_com[pat][node])\n",
    "        segment_com[pat][seg_num] = np.array(segment_com[pat][seg_num]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe22b00-7fb6-4643-a1f2-c7ea52f4f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the slice UIDs from 'segment_slices' and associates them to a specific dicom file. \n",
    "# Those files are then stored in a new dictionary 'ct_slices'\n",
    "ct_slices_tmp = {}\n",
    "ct_slices = {}\n",
    "selected_rows = []\n",
    "\n",
    "for pat, df_group in tqdm(meta_ct_df.groupby(\"Subject ID\")):\n",
    "    if pat not in list(segment_slices.keys()): continue\n",
    "    ct_slices_tmp[pat] = {}\n",
    "    ct_slices[pat] = {}\n",
    "    for idx, ct_row in df_group[df_group.Modality == \"CT\"].iterrows():\n",
    "        ct_dir = data_path.joinpath('ct_dicom').joinpath(ct_row[\"File Location\"].replace('\\\\','/'))\n",
    "        for node in segment_slices[pat].keys():\n",
    "            ct_slices_tmp[pat][node] = []\n",
    "            ct_slices[pat][node] = []\n",
    "            \n",
    "        for ct_file in ct_dir.glob('*'):\n",
    "            if '.xml' in ct_file.as_posix(): continue\n",
    "            ct_slice = pydicom.dcmread(ct_file)\n",
    "            instance_uid = ct_slice.SOPInstanceUID\n",
    "            for node in segment_slices[pat].keys():\n",
    "                if instance_uid in segment_slices[pat][node]:\n",
    "                    ct_slices_tmp[pat][node].append(ct_file.with_suffix('').name.split('-')[-1])\n",
    "        for node in ct_slices_tmp[pat].keys():\n",
    "            ct_slices_tmp[pat][node] = sorted(ct_slices_tmp[pat][node])\n",
    "            ct_slices[pat][node] = ct_slices_tmp[pat][node]\n",
    "            if len(ct_slices_tmp[pat][node]) < 1:\n",
    "                ct_slices[pat].pop(node)\n",
    "\n",
    "ct_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724cf74f-97cb-497d-8f9e-395a0e206f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the dictionary containing dicom files for each node\n",
    "with open(\"ct_slices_dict.pkl\", 'wb') as f:\n",
    "    pickle.dump(ct_slices, f)\n",
    "    pickle.dump(segment_slices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb9507-420b-44f9-99a4-f8cf295d4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the slices for each node and save in NifTi format\n",
    "problem_images = []\n",
    "for patient in tqdm(ct_slices.keys()):\n",
    "    #if int(patient.split('-')[-1]) < 315: continue\n",
    "    pat_df = meta_ct_df.loc[[patient]]\n",
    "    patient_nii_path = nii_path.joinpath(patient)\n",
    "    patient_nii_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    #Convert the CT Image\n",
    "    ct_row = pat_df[pat_df[\"Modality\"] == \"CT\"]\n",
    "    ct_directory = data_path.joinpath('ct_dicom').joinpath(ct_row[\"File Location\"].iloc[0].replace('\\\\','/'))\n",
    "    ct_image = read_dicom_image(ct_directory)\n",
    "    axial_size = ct_image.GetDepth()\n",
    "    print(axial_size)\n",
    "    for node in ct_slices[patient].keys():\n",
    "        output_file = patient_nii_path.joinpath(f\"image_{node}.nii.gz\")\n",
    "        node_slices_tmp = [int(idx) for idx in ct_slices[patient][node]]\n",
    "        slices = axial_size - np.array(node_slices_tmp)\n",
    "        print(patient, node)\n",
    "        print(ct_slices[patient][node])\n",
    "        print(slices)\n",
    "        print(ct_image.GetSize())\n",
    "        modified_image = ct_image[:,:, slices.min():slices.max()+1]\n",
    "        print(modified_image.GetSize())\n",
    "        if modified_image.GetSize()[-1] < 1:\n",
    "            problem_images.append((patient, node))\n",
    "            continue\n",
    "        sitk.WriteImage(modified_image, str(output_file))\n",
    "        \n",
    "        #print(ct_image.GetSize())\n",
    "        #print(modified_image.GetSize())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55169baf-080b-4506-8955-444b07de69a1",
   "metadata": {},
   "source": [
    "## Embedding calculation\n",
    "The following blocks calculate foundation embeddings using the Harvard AIM CT foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a15b5-710b-495e-a38c-9f370f02c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lighter_zoo import SegResEncoder\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImage, EnsureType, Orientation,\n",
    "    ScaleIntensityRange, CropForeground\n",
    ")\n",
    "from monai.inferers import SlidingWindowInferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c42b6-726f-46f7-8d07-dfc7aed2be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the foundation model as a feature extractor, and prepare the preprocessing step\n",
    "model = SegResEncoder.from_pretrained(\n",
    "    \"project-lighter/ct_fm_feature_extractor\"\n",
    ")\n",
    "model.eval()\n",
    "preprocess = Compose([\n",
    "    LoadImage(ensure_channel_first=True),  # Load image and ensure channel dimension\n",
    "    EnsureType(),                         # Ensure correct data type\n",
    "    Orientation(axcodes=\"SPL\"),           # Standardize orientation\n",
    "    # Scale intensity to [0,1] range, clipping outliers\n",
    "    ScaleIntensityRange(\n",
    "        a_min=-1024,    # Min HU value\n",
    "        a_max=2048,     # Max HU value\n",
    "        b_min=0,        # Target min\n",
    "        b_max=1,        # Target max\n",
    "        clip=True       # Clip values outside range\n",
    "    ),\n",
    "    CropForeground()    # Remove background to reduce computation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11843582-246e-495d-a3f1-6239a159538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through dictionary of CT slices and calucate the embedding. Embeddings are dumped into a pickle file, one per node\n",
    "\n",
    "\n",
    "lung_embeddings = {}\n",
    "\n",
    "for patient in tqdm(ct_slices.keys()):\n",
    "    lung_embeddings[patient] = {}\n",
    "    if int(patient.split('-')[-1]) < 340: continue\n",
    "    print(patient)\n",
    "    patient_emb_path = embedding_path.joinpath(patient)\n",
    "    patient_emb_path.mkdir(exist_ok=True, parents=True)\n",
    "    for node in ct_slices[patient].keys():\n",
    "        print(f'    {node}')\n",
    "        input_path = nii_path.joinpath(patient).joinpath(f'image_{node}.nii.gz')\n",
    "        input_tensor = preprocess(input_path.as_posix())\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor.unsqueeze(0))[-1]\n",
    "\n",
    "            # Average pooling compressed the feature vector across all patches. If this is not desired, remove this line and \n",
    "            # use the output tensor directly which will give you the feature maps in a low-dimensional space.\n",
    "            avg_output = torch.nn.functional.adaptive_avg_pool3d(output, 1).squeeze()\n",
    "            #lung_embeddings[patient][node] = avg_output\n",
    "            with open(patient_emb_path.joinpath(f\"embedding_{patient}_{node}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(avg_output, f)\n",
    "                f.close()\n",
    "        del(input_tensor)\n",
    "        del(avg_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8714e-d2eb-47a6-80ec-7be9297f6d05",
   "metadata": {},
   "source": [
    "## Resampling code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd91361-456c-48b1-b4ce-206bfa00bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler = sitk.ResampleImageFilter()\n",
    "resampler.SetOutputDirection([1, 0, 0, 0, 1, 0, 0, 0, 1])\n",
    "resampling = [1,1,1]\n",
    "resampler.SetOutputSpacing(resampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93441f56-0273-44ef-bd0c-03e62ed3b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bouding_boxes(ct, pt):\n",
    "    \"\"\"\n",
    "    Get the bounding boxes of the CT and PT images.\n",
    "    This works since all images have the same direction\n",
    "    \"\"\"\n",
    "\n",
    "    ct_origin = np.array(ct.GetOrigin())\n",
    "    pt_origin = np.array(pt.GetOrigin())\n",
    "\n",
    "    ct_position_max = ct_origin + np.array(ct.GetSize()) * np.array(\n",
    "        ct.GetSpacing())\n",
    "    pt_position_max = pt_origin + np.array(pt.GetSize()) * np.array(\n",
    "        pt.GetSpacing())\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            np.maximum(ct_origin, pt_origin),\n",
    "            np.minimum(ct_position_max, pt_position_max),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4783ae7-4109-4445-96e6-fdfb4f46c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_one_patient(p):\n",
    "    pat_str = p.as_posix().split('/')[-1]\n",
    "    patient_resample_path = resample_path.joinpath(pat_str)\n",
    "    patient_resample_path.mkdir(exist_ok=True, parents=True)\n",
    "    try:\n",
    "        ct = sitk.ReadImage(p.joinpath('image.nii.gz').as_posix())\n",
    "    except:\n",
    "        print(f\"    unable to read image file for {pat_str}\")\n",
    "        #os.rmdir(p)\n",
    "        #os.rmdir(patient_resample_path)\n",
    "        #print(f\"{pat_str} folder removed due to being empty\")\n",
    "        return\n",
    "    #label = sitk.ReadImage(os.path.join(savePath, p, 'mask_GTVp.nii.gz'))\n",
    "    bb = get_bouding_boxes(ct, ct)\n",
    "    size = np.round((bb[3:] - bb[:3]) / resampling).astype(int)\n",
    "    resampler.SetOutputOrigin(bb[:3])\n",
    "    resampler.SetSize([int(k) for k in size])  # sitk is so stupid\n",
    "    resampler.SetInterpolator(sitk.sitkBSpline)\n",
    "    ct = resampler.Execute(ct)\n",
    "\n",
    "    #sitk.WriteImage(ct, patient_resample_path.joinpath('image.nii.gz').as_posix())\n",
    "    resampler.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "\n",
    "    mask_sizes = []\n",
    "    for m in p.glob('*.nii.gz'):\n",
    "        if 'image' in str(m): continue\n",
    "        label = sitk.ReadImage(m.as_posix())\n",
    "        label = resampler.Execute(label)\n",
    "\n",
    "        label_array = sitk.GetArrayViewFromImage(label)\n",
    "        label_locations = np.where(label_array > 0)\n",
    "        mask_sizes.append(np.max(label_locations, axis=1) - np.min(label_locations, axis=1))\n",
    "        #sitk.WriteImage(label, patient_resample_path.joinpath(m.as_posix().split('/')[-1]).as_posix())\n",
    "    return mask_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba1bde-affc-4859-a6ce-8a1972f64dea",
   "metadata": {},
   "source": [
    "## 4. Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c8089-000f-415e-86b6-638e3dd46609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_range(min_d, max_d, d, size_d, p):\n",
    "    min_pad = 0\n",
    "    max_pad = 0\n",
    "    if min_d<0:\n",
    "        min_pad = abs(min_d)\n",
    "        min_d = 0\n",
    "        #max_d = min_d + d\n",
    "        #if max_d - size_d > 0:\n",
    "        #    max_pad = max_d-size_d\n",
    "            \n",
    "        #assert (max_d<size_d), f\"Cannot extract the patch with the shape {size_d} from the image with the shape {d} for patient {p}.\"\n",
    "    \n",
    "    if max_d>d:\n",
    "        max_pad = max_d - d\n",
    "        max_d = d\n",
    "        #min_d = max_d - size_d\n",
    "        #if min_d < 0:\n",
    "        #    min_pad = abs(min_d)\n",
    "            \n",
    "        #assert (min_d>0), f\"Cannot extract the patch with the shape {size_d} from the image with the shape {d} for patient {p}.\"\n",
    "\n",
    "    return min_d, max_d, int(min_pad), int(max_pad)\n",
    "physical_locations = {}\n",
    "for p_dir in tqdm(list(resample_path.glob('*'))):\n",
    "    p_str = p_dir.as_posix().split('/')[-1]\n",
    "    print(p_str)\n",
    "    #if p_str not in patients_to_retry: continue\n",
    "    #try:\n",
    "    #if p_str in patients_to_drop:\n",
    "    #    print('skip ', p_str)\n",
    "    #    continue\n",
    "    patient_patch_path = patch_path.joinpath(p_str)\n",
    "    patient_patch_path.mkdir(exist_ok=True, parents=True)\n",
    "    physical_locations[p_str] = {}\n",
    "    patch_size = np.array([80,80,80])\n",
    "    for m in p_dir.glob('*.nii.gz'):\n",
    "        print('-----------------')\n",
    "        m_str = m.as_posix().split('/')[-1]\n",
    "        if 'image' in m_str: continue\n",
    "        #try:\n",
    "        image = sitk.ReadImage(p_dir.joinpath('image.nii.gz').as_posix())\n",
    "        mask = sitk.ReadImage(m.as_posix())\n",
    "        print(m_str)\n",
    "        #crop the image to patch_size around the tumor center\n",
    "        tumour_center, center_location = find_centroid(mask, p_str) # center of GTV\n",
    "        size = patch_size\n",
    "        min_coords = np.floor(tumour_center - size / 2).astype(np.int64)\n",
    "        max_coords = np.floor(tumour_center + size / 2).astype(np.int64)\n",
    "        min_x, min_y, min_z = min_coords\n",
    "        max_x, max_y, max_z = max_coords\n",
    "        (img_x, img_y, img_z)=image.GetSize()\n",
    "        min_x, max_x, min_pad_x, max_pad_x = tune_range(min_x, max_x, img_x, size[0], p_str) \n",
    "        min_y, max_y, min_pad_y, max_pad_y = tune_range(min_y, max_y, img_y, size[1], p_str) \n",
    "        min_z, max_z, min_pad_z, max_pad_z = tune_range(min_z, max_z, img_z, size[2], p_str) \n",
    "\n",
    "        min_pad = int(max([min_pad_x, min_pad_y, min_pad_z]))\n",
    "        max_pad = int(max([max_pad_x, max_pad_y, max_pad_z]))\n",
    "        lpad = list([min_pad_x, min_pad_y, min_pad_z])\n",
    "        upad = list([max_pad_x, max_pad_y, max_pad_z])\n",
    "        #print(m_str)\n",
    "        #print(lpad)\n",
    "        #print(upad)\n",
    "        print(image.GetSize())\n",
    "        print(min_coords, max_coords)\n",
    "        print(min_pad, max_pad)\n",
    "        image = image[min_x:max_x, min_y:max_y, min_z:max_z]\n",
    "        # window image intensities to [-500, 1000] HU range\n",
    "        image = sitk.Clamp(image, sitk.sitkFloat32, -500, 500)\n",
    "        mask = mask[min_x:max_x, min_y:max_y, min_z:max_z]\n",
    "        print(image.GetSize())\n",
    "        image = sitk.ConstantPad(image, lpad, upad, 0.0)\n",
    "        mask = sitk.ConstantPad(mask, lpad, upad, 0.0)\n",
    "        print(image.GetSize())\n",
    "        sitk.WriteImage(image, patient_patch_path.joinpath(f\"image_{m_str.replace('Struct_','')}\").as_posix())\n",
    "        sitk.WriteImage(mask, patient_patch_path.joinpath(m_str).as_posix())\n",
    "        physical_locations[p_str][m_str.replace('Struct_','').replace('.nii.gz','')] = center_location\n",
    "        del(image)\n",
    "        del(mask)\n",
    "        #except:\n",
    "        #    print(m)\n",
    "        #    raise Exception('something went wrong...')\n",
    "    \n",
    "    #except:\n",
    "    #    print(p_str)\n",
    "        \n",
    "with open(patch_path.joinpath('locations.pkl'), 'wb') as f:\n",
    "    pickle.dump(physical_locations, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36646e-b9b0-4c07-a19f-e3b64c647fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_patch_paths = patch_path.glob('*/')\n",
    "tumor_locations = pd.read_pickle(location_pickle_path)\n",
    "centered_locations = {}\n",
    "no_gtvp = []\n",
    "for pat in tqdm(patient_patch_paths):\n",
    "    pat_str = pat.as_posix().split('/')[-1]\n",
    "    if 'locations' in pat_str: continue\n",
    "    if 'no_gtvp' in pat_str: continue\n",
    "    print(pat_str)\n",
    "    centered_locations[pat_str] = {}\n",
    "    n_tumors = len(tumor_locations[pat_str])\n",
    "    translation_factor = np.array([0., 0., 0.])\n",
    "    if n_tumors == 1:\n",
    "        if 'GTVp' in tumor_locations[pat_str].keys():\n",
    "            centered_locations[pat_str]['GTVp'] = np.array([0., 0., 0.])\n",
    "        else:\n",
    "            centered_locations[pat_str][next(iter(tumor_locations[pat_str].keys()))] = np.array([0., 0., 0.])\n",
    "            no_gtvp.append(pat_str)\n",
    "        continue\n",
    "    else:\n",
    "        gtvs = tumor_locations[pat_str].keys()\n",
    "        print(f\"    {tumor_locations[pat_str].keys()}\")\n",
    "        if 'GTVp' in tumor_locations[pat_str].keys():\n",
    "            translation_factor = tumor_locations[pat_str]['GTVp']\n",
    "        else:\n",
    "            no_gtvp.append(pat_str)\n",
    "            print('    no GTVp, choosing highest GTVn in Z')\n",
    "            array_locs = np.array([val for val in tumor_locations[pat_str].values()])\n",
    "            origin_idx = np.where(array_locs == np.max(array_locs, axis=0)[2])[0][0]\n",
    "            translation_factor = array_locs[origin_idx]\n",
    "    for tumor in tumor_locations[pat_str]:\n",
    "        centered_locations[pat_str][tumor.replace('.nii.gz','')] = tumor_locations[pat_str][tumor] - translation_factor\n",
    "\n",
    "#with open(edge_path.joinpath('centered_locations_radcure_100324.pkl'), 'wb') as f:\n",
    "#    pickle.dump(centered_locations, f)\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5283940-8880-4c17-84bb-3b6a8b3f5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_graphs = {}\n",
    "edge_dict = {}\n",
    "for pat in centered_locations.keys():\n",
    "    patient_graphs[pat] = nx.DiGraph(directed=True)\n",
    "    edges_for_nx = []\n",
    "    nodes = list(centered_locations[pat].keys())\n",
    "    node_pos = list(centered_locations[pat].values())\n",
    "    n_nodes = len(nodes)\n",
    "    if n_nodes < 2:\n",
    "        edges_for_nx.extend([(nodes[0], nodes[0])])\n",
    "    else:\n",
    "        n_neighbors = n_nodes-1 if n_nodes <= 3 else 3\n",
    "        edge_list = sklearn.neighbors.kneighbors_graph(node_pos, n_neighbors).toarray()\n",
    "        for node_idx, node_name in enumerate(nodes):\n",
    "            #edges_for_nx.extend([(nodes[node_idx], nodes[jdx]) for jdx in range(len(edge_list[node_idx])) if edge_list[node_idx][jdx]])\n",
    "            edges_for_nx.extend([(nodes[node_idx], nodes[jdx]) for jdx in range(len(edge_list[node_idx]))])\n",
    "\n",
    "    patient_graphs[pat].add_edges_from(edges_for_nx)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a40157-57c2-4adf-8acd-04aec2b71313",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_graphs['RADCURE-0006'].edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b45ce-b84d-429e-81d6-7eaadf359cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(edge_path.joinpath('proto_complete_graphs_100424.pkl'), 'wb') as f:\n",
    "    pickle.dump(patient_graphs, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96adce-7c38-4b5c-a31b-4d0b8c083eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172f9f1-b999-42d3-ae68-09d520f8bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_dict = {}\n",
    "for pat in tqdm(list(nii_path.glob('*'))):\n",
    "    pat_str = pat.as_posix().split('/')[-1]\n",
    "    for m in pat.glob('*.nii.gz'):\n",
    "        if 'image' in str(m):\n",
    "            continue\n",
    "        m_str = m.as_posix().split('/')[-1].strip('.nii.gz').strip('Struct_')\n",
    "        key_name = f\"{pat_str}__{m_str}\"\n",
    "        rad_dict[key_name] = {}\n",
    "        rad_dict[key_name]['Image'] = m.as_posix().replace(m.as_posix().split('/')[-1], 'image.nii.gz')\n",
    "        rad_dict[key_name]['Mask'] = m.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b9184-f09a-4f14-a04b-f0f543e09b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_df = pd.DataFrame.from_dict(rad_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f24b73-147c-48d6-99e2-c3bf53ca1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(range(0,16000, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491c9a4-559b-4a49-8ae8-8fbb19c312e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_df.iloc[15000:16000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a71d3-faef-4398-896a-2dc52ad87bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca48e3-bb58-4b5c-8dfc-823d959396b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "for idx in range(8000, 9000, 1000):\n",
    "    rad_df.iloc[idx:idx+1000].to_csv(data_path.joinpath('proto_radiomics.csv'))\n",
    "    command = [\n",
    "        \"pyradiomics\",\n",
    "        data_path.joinpath('proto_radiomics.csv').as_posix(),\n",
    "        \"-o\", data_path.joinpath(f\"radiomics_part_{idx}.csv\").as_posix(),\n",
    "        \"-f\", \"csv\",\n",
    "        \"--param\", './hnc_project/radiomics/pyradiomics_param.yaml',\n",
    "    ]\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94959813-ac91-4647-befb-54c5fffcb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a9bd6-2bbc-4a5c-89d9-aac9fef5cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiomics.setVerbosity(20)\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor()\n",
    "extractor.enableImageTypeByName('Wavelet')\n",
    "print(extractor.settings)\n",
    "print(extractor.enabledImagetypes)\n",
    "print(extractor.enabledFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195722a0-ca76-4c91-a87d-3e3a862f519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_patch_paths = patch_path.glob('*/')\n",
    "for pat in patient_patch_paths:\n",
    "    pat_str = pat.as_posix().split('/')[-1]\n",
    "    print(pat_str)\n",
    "\n",
    "    patches = pat.glob('image*.nii.gz')\n",
    "    features_to_keep = {}\n",
    "    for p in patches:\n",
    "        p_name = p.as_posix().split('_')[-1].replace('.nii.gz','')\n",
    "        print(f\"    {p_name}\")\n",
    "        image = p.as_posix()\n",
    "        mask = p.as_posix().replace('image', 'Struct')\n",
    "        features = extractor.execute(image, mask)\n",
    "        features_to_keep[p_name] = {key: value for key, value in features.items() if key.startswith('original')}\n",
    "        \n",
    "    with open(radiomics_path.joinpath(f\"features_{pat_str}.pkl\"), 'wb') as f:\n",
    "        pickle.dump(features_to_keep, f)        \n",
    "        f.close()\n",
    "      \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-v3",
   "language": "python",
   "name": "pytorch_gpu_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
